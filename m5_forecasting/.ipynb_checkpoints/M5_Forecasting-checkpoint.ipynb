{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Forecasting - Accuracy\n",
    "\n",
    "This notebook is about the Kaggle competition [M5 Forecasting - Accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy) which is about forcasting Walmart sales. Detailed information about this competition can be found in the [M5 Participants Guide](https://mofc.unic.ac.cy/m5-competition/).\n",
    "\n",
    "I want to clarify upfront, that my scope for working on this competition is quite limited - I will primarily use the competition and its dataset to revisit time series forecasting, which is an area I have not had the opportunity to work in much recently. In particular, I will skip exploratory data analysis and jump into model training almost using the sales time series data only straight away. I am planning to experiment with solution approaches involving [Holt-Winterâ€™s Seasonal Exponential Smoothing](https://otexts.com/fpp2/holt-winters.html) (also known as *Triple Exponential Smoothing*), which is a rather simple but effective method for time series forecasting. \n",
    "\n",
    "Without going into further details, the competition uses a Weighted Root Mean Squared Scaled Error (WRMSSE) metric for rating the forecasting submissions, i.e. the submission ratings are always positives and the closer to zero the better. At the time of creating this notebook (end of May 2020), the leaderboard's top 10 has achieved scores below 0.45. I am curious to see what I can achieve with relatively low effort and rather simple solution approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1919 entries, id to d_1913\n",
      "dtypes: int64(1913), object(6)\n",
      "memory usage: 446.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# import sales data\n",
    "sales = pd.read_csv('data/sales_train_validation.csv')\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales data comes with one column for every day, which is not a nice format for data analysis. Transforming the data into a clean long format will be more convenient. But before doing so, I will remove the redundant columns *dept_id*, *cat_id*, *store_id* and *state_id*. In case I will need them later (which I don't expect), I will extract them into a separate DataFrame, as retrieving them that way will be more comfortable than reconstructing them individually from the *id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id variable  value\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001      d_1      0\n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002      d_1      0\n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003      d_1      0\n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004      d_1      0\n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005      d_1      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract and remove redundant columns\n",
    "sales_meta = sales[['id', 'item_id', 'dept_id', \n",
    "                    'cat_id', 'store_id', 'state_id']]\n",
    "sales = sales.drop(columns=['dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "\n",
    "# Transforming sales data into long format\n",
    "day_columns = ['d_' + str(i) for i in range(1, 1914)]\n",
    "sales = sales.melt(id_vars=['id', 'item_id'],\n",
    "                  value_vars=day_columns)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58327370 entries, 0 to 58327369\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   id        object\n",
      " 1   item_id   object\n",
      " 2   variable  object\n",
      " 3   value     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though I will not need them for the scope of my basic forecasting approaches, I will also import and show the other two files included in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
       "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
       "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
       "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  \n",
       "2          NaN          NaN          NaN        0        0        0  \n",
       "3          NaN          NaN          NaN        1        1        0  \n",
       "4          NaN          NaN          NaN        1        0        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dates data\n",
    "dates = pd.read_csv('data/calendar.csv')\n",
    "dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id        item_id  wm_yr_wk  sell_price\n",
       "0     CA_1  HOBBIES_1_001     11325        9.58\n",
       "1     CA_1  HOBBIES_1_001     11326        9.58\n",
       "2     CA_1  HOBBIES_1_001     11327        8.26\n",
       "3     CA_1  HOBBIES_1_001     11328        8.26\n",
       "4     CA_1  HOBBIES_1_001     11329        8.26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import prices data\n",
    "prices = pd.read_csv('data/sell_prices.csv')\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Exponential Smoothing models\n",
    "\n",
    "As explained at the beginning of the notebook, I will experiment with solution approaches involving Holt-Winterâ€™s Seasonal Exponential Smoothing. In particular, I will pursue and compare the following approaches:\n",
    "\n",
    "* Fit one model each (3049 models) to the sales per day and item in the first Walmart store in the dataset (CA_1) and repeat the results for the other stores\n",
    "* Fit one model each (3049 models) to the mean sales per day per item_id and use these predictions for all stores\n",
    "* Enhance the previous approach by adjusting (scaling) the predictions for each ID by the mean sales in the last 70 days\n",
    "* Fit one model each (30490 models) to the sales per day for each individual ID (3049 items * 10 stores)\n",
    "\n",
    "For every model, I will use 7 as seasonal period, corresponding to the different days of the week. Also, I will limit the training data used to the last 70 days to save a vast amount of runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IDs: 30490\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "id_list = sales.id.unique()\n",
    "# Additional evaluation IDs required for submission file\n",
    "id_list_eval = np.array([s.replace('validation', 'evaluation') for s in id_list])\n",
    "print('Number of IDs:', len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(results, prefix, repeat_for_eval=True):\n",
    "    \"\"\" Helper function to write submissions to a proper CSV submission file. \n",
    "    The function supports both dictionaries and lists as results input:\n",
    "    - Dictionaries must contain one prediction for each ID from id_list.\n",
    "      For repeat_for_eval=False, it must also include one prediction for \n",
    "      each ID from id_list_eval. \n",
    "    - Lists will simply be repeated for each ID of id_list + id_list_eval. \n",
    "      This matches the desired behaviour for predicting per item_id.\n",
    "    \"\"\"\n",
    "    if not (isinstance(results, dict) or isinstance(results, list)):\n",
    "        print('Unsupported results type:', type(results))\n",
    "        return None\n",
    "    \n",
    "    with open(prefix + '_submission.csv', 'w+') as fout:\n",
    "        # write header\n",
    "        fout.write('id,F1,F2,F3,F4,F5,F6,F7,F8,F9,F10,'\n",
    "                   'F11,F12,F13,F14,F15,F16,F17,F18,F19,'\n",
    "                   'F20,F21,F22,F23,F24,F25,F26,F27,F28\\n')\n",
    "        if isinstance(results, dict):\n",
    "            if repeat_for_eval:\n",
    "                for id in id_list:\n",
    "                    fout.write(id + ',' + ','.join(map(str, results[id])) + '\\n')\n",
    "                for n, id in enumerate(id_list_eval):\n",
    "                    fout.write(id + ',' + ','.join(map(str, results[id_list[n]])) + '\\n')\n",
    "            else:\n",
    "                for id in np.concatenate([id_list, id_list_eval]):\n",
    "                    fout.write(id + ',' + ','.join(map(str, results[id])) + '\\n')\n",
    "        if isinstance(results, list):\n",
    "            for n, id in enumerate(np.concatenate([id_list, id_list_eval])):\n",
    "                fout.write(id + ',' + ','.join(map(str, results[n % len(results)])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2134300 entries, 0 to 2134299\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   id        object\n",
      " 1   item_id   object\n",
      " 2   variable  object\n",
      " 3   value     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 65.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Preparing reduced dataset\n",
    "last_70_days = ['d_' + str(i) for i in range(1844, 1914)]\n",
    "sales_70d = sales[sales.variable.isin(last_70_days)].reset_index(drop=True)\n",
    "sales_70d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Exponential Smoothing for store CA_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc628ee1ae42278de45620545df323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3049.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Holt-Winterâ€™s for each CA_1 ID\n",
    "ses_results_ca_1 = []\n",
    "df = sales_70d[['id', 'value']]\n",
    "\n",
    "for id in tqdm(id_list[:3049]):\n",
    "    train = df[df.id == id].value.values\n",
    "    model = ExponentialSmoothing(train, trend=\"add\", \n",
    "                                 seasonal=\"add\", \n",
    "                                 seasonal_periods=7)\n",
    "    fit = model.fit(optimized=True)\n",
    "    pred = fit.forecast(28).clip(0, np.inf).round().astype('int')\n",
    "    ses_results_ca_1.append(pred)\n",
    "    \n",
    "save_submission(ses_results_ca_1, 'ses_ca_1_70d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission scored 1.60218 on Kaggle, which is not great, but at least a first value to benchmark against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Exponential Smoothing for per-day-means per item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>item_mean_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>d_1844</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>d_1844</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>d_1844</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>d_1844</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>d_1844</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id variable  value  item_mean_1d\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001   d_1844      0           0.2\n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002   d_1844      0           0.0\n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003   d_1844      0           0.1\n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004   d_1844      0           0.9\n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005   d_1844      2           0.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare means per item_id\n",
    "sales_70d['item_mean_1d'] = sales_70d['value'].groupby(\n",
    "    [sales_70d['variable'], sales_70d['item_id']]).transform('mean')\n",
    "item_ids = sales_70d.item_id.unique()\n",
    "sales_70d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed2b5846c48403dbee32882c5ae39ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3049.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Holt-Winterâ€™s for the means per item_id\n",
    "ses_results_item_means = []\n",
    "ses_results_item_means_raw = []\n",
    "df = sales_70d[['item_id', 'variable', 'item_mean_1d']].drop_duplicates()\n",
    "\n",
    "for id in tqdm(item_ids):\n",
    "    train = df[df.item_id == id]['item_mean_1d'].values\n",
    "    model = ExponentialSmoothing(train, trend=\"add\", \n",
    "                                 seasonal=\"add\", \n",
    "                                 seasonal_periods=7)\n",
    "    fit = model.fit(optimized=True)\n",
    "    pred_raw = fit.forecast(28).clip(0, np.inf)\n",
    "    ses_results_item_means_raw.append(pred_raw)\n",
    "    pred = pred_raw.round().astype('int')\n",
    "    ses_results_item_means.append(pred)\n",
    "    \n",
    "save_submission(ses_results_item_means, 'ses_item_means_70d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scored 1.05642 on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling by adjusting by per-store mean sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mean_70d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>1.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1.185714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  mean_70d\n",
       "0  HOBBIES_1_001_CA_1_validation  1.014286\n",
       "1  HOBBIES_1_002_CA_1_validation  0.200000\n",
       "2  HOBBIES_1_003_CA_1_validation  0.485714\n",
       "3  HOBBIES_1_004_CA_1_validation  1.857143\n",
       "4  HOBBIES_1_005_CA_1_validation  1.185714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing last 70 day means per id\n",
    "sales_70d['mean_70d'] = sales_70d['value'].groupby(sales_70d['id']).transform('mean')\n",
    "means_70d_per_id = sales_70d[['id', 'mean_70d']].drop_duplicates().reset_index(drop=True)\n",
    "means_70d_per_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_mean_70d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>0.541429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>0.255714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>0.272857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>1.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>1.031429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id  item_mean_70d\n",
       "0  HOBBIES_1_001       0.541429\n",
       "1  HOBBIES_1_002       0.255714\n",
       "2  HOBBIES_1_003       0.272857\n",
       "3  HOBBIES_1_004       1.628571\n",
       "4  HOBBIES_1_005       1.031429"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing last 70 day means per item_id\n",
    "sales_70d['item_mean_70d'] = sales_70d['value'].groupby(sales_70d['item_id']).transform('mean')\n",
    "mean_item_sales_70d = sales_70d[['item_id', 'item_mean_70d']].drop_duplicates().reset_index(drop=True)\n",
    "mean_item_sales_70d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c57d8fbfe6047a29ed048448b2c1d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# constructing and saving dictionary with adjusted predictions\n",
    "ses_results_item_means_adjusted = {}\n",
    "\n",
    "for n, id in tqdm(list(enumerate(id_list))):\n",
    "    if mean_item_sales_70d.iloc[n % 3049, 1] > 0:\n",
    "        adj = means_70d_per_id.loc[means_70d_per_id.id == id, 'mean_70d'].values[0] \\\n",
    "            / mean_item_sales_70d.iloc[n % 3049, 1]\n",
    "    else:\n",
    "        adj = 0.0\n",
    "    pred = adj * ses_results_item_means_raw[n % 3049]\n",
    "    ses_results_item_means_adjusted[id] = pred.clip(0, np.inf).round().astype('int')\n",
    "\n",
    "save_submission(ses_results_item_means_adjusted, 'ses_item_means_adj_70d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scored 0.81436 on Kaggle, a nice improvement from 1.05 without any additional model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Exponential Smoothing per id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af28e44d52324050ab75a2246172dcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Holt-Winterâ€™s for each individual ID\n",
    "ses_results_per_id = {}\n",
    "df = sales_70d[['id', 'value']].copy()\n",
    "\n",
    "for id in tqdm(id_list):\n",
    "    train = df[df.id == id].value.values\n",
    "    model = ExponentialSmoothing(train, trend='add', \n",
    "                                 seasonal='add', \n",
    "                                 seasonal_periods=7)\n",
    "    fit = model.fit(optimized=True)\n",
    "    pred = fit.forecast(28).clip(0, np.inf).round().astype('int')\n",
    "    ses_results_per_id[id] = pred\n",
    "    \n",
    "save_submission(ses_results_per_id, 'ses_per_id_70d')\n",
    "\n",
    "with open('ses_results_per_id_70d.pkl', 'wb') as handle:\n",
    "    pickle.dump(ses_results_per_id, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scored 0.74177 on Kaggle, really not bad at all! This is less than half the score of the top-1 model, quite nice for the simplicity of the chosen appraoch in my opinion.\n",
    "\n",
    "As a last attempt for improvement, I want to repeat the same approach but with using multiplicative trend and seasonal components where possible. This can only be done for time series that are strictly greater than zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83660b04323a4682ad3f1c2f3cf2c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Holt-Winterâ€™s for each individual ID with multiplicative \n",
    "# trend and seasonal components where possible\n",
    "import copy\n",
    "ses_results_per_id_mul = copy.deepcopy(ses_results_per_id)\n",
    "\n",
    "for id in tqdm(id_list):\n",
    "    train = df[df.id == id].value.values\n",
    "    if np.all(train > 0.0):\n",
    "        model = ExponentialSmoothing(train, trend='mul', \n",
    "                                     seasonal='mul', \n",
    "                                     seasonal_periods=7)\n",
    "        fit = model.fit(optimized=True)\n",
    "        pred = fit.forecast(28).clip(0, np.inf).round().astype('int')\n",
    "        ses_results_per_id_mul[id] = pred\n",
    "    \n",
    "save_submission(ses_results_per_id_mul, 'ses_per_id_70d_mul')\n",
    "\n",
    "with open('ses_results_per_id_70d_mul.pkl', 'wb') as handle:\n",
    "    pickle.dump(ses_results_per_id_mul, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission scored 464535.75364 on Kaggle. Huh? Certainly something must have gone wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOODS_3_090_CA_4_validation [-2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648]\n",
      "FOODS_3_586_TX_2_validation [-2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648 -2147483648 -2147483648\n",
      " -2147483648 -2147483648 -2147483648 -2147483648]\n"
     ]
    }
   ],
   "source": [
    "for id in ses_results_per_id_mul:\n",
    "    if ses_results_per_id_mul[id].min() < -1e3 or ses_results_per_id_mul[id].max() > 1e3:\n",
    "        print(id, ses_results_per_id_mul[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this doesn't look right - obviously some form of numeric issues and a good example why predictions should never be trusted blindly. For the scope of this notebook, I will just replace the predictions for both IDs with the ones from the previous run and re-submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_results_per_id_mul['FOODS_3_090_CA_4_validation'] = ses_results_per_id['FOODS_3_090_CA_4_validation']\n",
    "ses_results_per_id_mul['FOODS_3_586_TX_2_validation'] = ses_results_per_id['FOODS_3_586_TX_2_validation']\n",
    "\n",
    "save_submission(ses_results_per_id_mul, 'ses_per_id_70d_mul')\n",
    "with open('ses_results_per_id_70d_mul.pkl', 'wb') as handle:\n",
    "    pickle.dump(ses_results_per_id_mul, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the submission scored 0.74974 on Kaggle, worse than the submission with additive components only. So the multiplicative components did not help at all. Well it was worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Exponential Smoothing per id (365 days)\n",
    "\n",
    "Last but not least, I have also computed the 30490 models with additive components using 365 days of training data on a different machine in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ses_results_per_id_365d.pkl', 'rb') as handle:\n",
    "    ses_results_per_id = pickle.load(handle)\n",
    "\n",
    "save_submission(ses_results_per_id, 'ses_per_id_365d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scored 0.77611 on Kaggle, i.e. worse than the model trained with only 70 days. Quite astonishing, but not extremely surprising too. An explanation for this phenomenon could be that Holt Winter's model is too simple to really benefit from the additional information inside the older training data, in particular as my chosen seasonal component only has a 7-day period. Extracting additional information gain from older data would probably require a more complex model and/or more model tuning, which exceeds the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
